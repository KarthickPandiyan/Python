RDD in spark
Resilient
Distributed
Dataset

RDD is to process the big dataset
SC - Spark context give method to create RDD
Sc.Textfile to create RDD object
Sc.hdfs to create RDD for dataset
sc.hive RDD
sc.Jdbc for rdd
sc.casandra rdd
sc.json csv sequence files, object files rdd

RDD to transforming
common operation are 

Map - 1:1 relation 
flatmap - capablity to produce 
filter - trim out the information 
distinct - unique values in rdd
sample - sample on rdd
union, intersection, subtract, cartesian

Many RDD method accept funcation as parameter
rdd.map(lambda x: x*x)
 
RDD Actions
Collect - all the values in the dataset after transformation
count
countby value
take
top
reduce

Spark script wont do anything untill driver execute the actions are called

libraries to import in python first sparconf and sparkcontext



